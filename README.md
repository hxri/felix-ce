# ğŸ“½ï¸ Apparel to Video Generation Pipeline

A modular, extensible Python pipeline for generating realistic full-body videos of people wearing specified apparel in various settings. Integrates with multiple AI video generation models via FAL.ai.

**Key Feature:** Single-command model swapping to compare latency, quality, and performance across 8+ video generation models.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Supported Models](#supported-models)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Components](#components)
- [Configuration](#configuration)
- [Dashboard](#dashboard)
- [API Reference](#api-reference)
- [Development](#development)

---

## ğŸ¯ Overview

This project automates the creation of high-quality product/fashion videos by:

1. **Image Generation**: Uses nano-banana/edit to generate a realistic full-body person image based on reference photos and outfit descriptions
2. **Video Generation**: Converts the generated image into a 4-8 second video using multiple AI video models
3. **Model Comparison**: Logs latency and metadata for each model, enabling performance benchmarking
4. **Interactive Dashboard**: Streamlit-based dashboard to visualize and compare model performance

### Use Cases

- E-commerce product demos (clothing, accessories)
- Fashion lookbooks and styling guides
- Virtual try-on experiences
- AI model benchmarking and evaluation

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Person + Environment Attributes             â”‚
â”‚        (PersonAttributes, EnvironmentAttributes)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Image Pipeline       â”‚
        â”‚  (ImagePipeline)       â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
        â”‚ â€¢ Prompt building      â”‚
        â”‚ â€¢ nano-banana/edit     â”‚
        â”‚ â€¢ Face preservation    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Generated Person Image  â”‚
        â”‚  (outputs/images/...)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Video Pipeline        â”‚
        â”‚ (VideoPipeline)        â”‚
        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
        â”‚ â€¢ Model factory        â”‚
        â”‚ â€¢ Video service calls  â”‚
        â”‚ â€¢ Metadata logging     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Video Generation (8 Models)          â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
        â”‚  â€¢ veo3    â€¢ ltx      â€¢ kling         â”‚
        â”‚  â€¢ grok    â€¢ luma     â€¢ pika          â”‚
        â”‚  â€¢ seedance â€¢ hunyuan                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Output Videos + Metadata    â”‚
        â”‚ (outputs/videos/YYYY_MM_DD/)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Dashboard (Streamlit) â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚ â€¢ Latency comparison   â”‚
        â”‚ â€¢ Model benchmarking   â”‚
        â”‚ â€¢ Video preview        â”‚
        â”‚ â€¢ Export reports       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¬ Supported Models

| Model | Endpoint | Duration | Status |
|-------|----------|----------|--------|
| **Veo3** | `fal-ai/veo3/image-to-video` | 4, 6, 8s | âœ… Active |
| **LTX** | `fal-ai/ltx-2/image-to-video/fast` | Any (s) | âœ… Active |
| **Kling** | `fal-ai/kling-video/v2.6/pro/image-to-video` | Variable | âœ… Active |
| **Grok** | `xai/grok-imagine-video/image-to-video` | Variable | âœ… Active |
| **Luma Ray-2** | `fal-ai/luma-dream-machine/ray-2/image-to-video` | 5s | âœ… Active |
| **Pika** | `fal-ai/pika/v2.2/image-to-video` | Variable | âœ… Active |
| **Seedance** | `fal-ai/bytedance/seedance/v1.5/pro/image-to-video` | Variable | âœ… Active |
| **Hunyuan** | `fal-ai/hunyuan-video-v1.5/image-to-video` | Variable | âœ… Active |

**Add new models**: Create a new service file (e.g., `src/services/video_generation/new_model_service.py`) and register it in `VideoPipeline._init_video_service()`.

---

## ğŸš€ Installation

### Prerequisites

- Python 3.10+
- FAL.ai API key (sign up at [fal.ai](https://fal.ai))
- Linux/macOS (or WSL on Windows)

### Setup

1. **Clone and navigate:**
   ```bash
   cd /home/hxri/Documents/conscious/felix
   ```

2. **Create virtual environment (optional):**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set FAL API key:**
   ```bash
   export FAL_API_KEY="your_fal_api_key_here"
   ```

   Or create `.env` file:
   ```
   FAL_API_KEY=your_fal_api_key_here
   ```

---

## ğŸ“– Quick Start

### Generate a Video (Full Pipeline)

```bash
export FAL_API_KEY="your_key"
python3 -m scripts.run_full_pipeline
```

**Output:**
- Generated image: `outputs/images/YYYY_MM_DD/nano_banana_edit/img_<ts>.png`
- Generated video: `outputs/videos/YYYY_MM_DD/<model>/video_<ts>.mp4`
- Metadata: `outputs/videos/YYYY_MM_DD/<model>/meta_<ts>.json`

### Swap Video Model

Edit `scripts/run_full_pipeline.py`:

```python
# Line ~50
video_pipeline = VideoPipeline(video_model="luma")  # Change: veo3, ltx, kling, grok, luma, pika, seedance, hunyuan
```

Then re-run the pipeline.

### Launch Dashboard

```bash
streamlit run scripts/dashboard.py
```

Opens at `http://localhost:8501`

### Test Single Model

```bash
python3 -m scripts.test_nanobanana \
  --ref assets/pranay.png \
  --prompt "Portrait of a man smiling" \
  --num 1
```

---

## ğŸ“¦ Components

### 1. **Schemas** (`src/schemas/`)

Define request/response models using Pydantic.

- **`person.py`**: Person attributes (height, weight, gender, age)
- **`environment.py`**: Environment attributes (setting, visual cues, apparel type)
- **`generation.py`**: Image/video generation requests
- **`artifacts.py`**: Metadata and generation info

### 2. **Services** (`src/services/`)

#### Image Generation (`image_generation/`)

- **`nano_banana_service.py`**: Basic nano-banana image generation
- **`nano_banana_edit_service.py`**: nano-banana/edit for image-to-image with references

#### Video Generation (`video_generation/`)

Eight service classes, one per model:

- **`veo3_service.py`**: FAL Veo3 integration
- **`ltx_service.py`**: FAL LTX integration
- **`kling_service.py`**: Kling video integration
- **`grok_service.py`**: Grok Imagine integration
- **`luma_service.py`**: Luma Ray-2 integration
- **`pika_service.py`**: Pika v2.2 integration
- **`seedance_service.py`**: Seedance integration
- **`hunyuan_service.py`**: Hunyuan integration

Each implements:
- `_resolve_ref()`: Convert local paths to data URIs
- `generate_video()`: Call FAL model, download video, save metadata

#### Prompt Building (`prompt_builder/`)

- **`image_prompt_service.py`**: Constructs identity and outfit prompts

### 3. **Pipelines** (`services/pipelines/`)

Orchestrate multi-stage workflows.

- **`image_pipeline.py`**: 
  - Takes person attributes + references
  - Calls nano-banana/edit
  - Returns generated person image
  - **Features**: Face preservation, outfit reference support, combined prompt generation

- **`video_pipeline.py`**: 
  - Factory pattern for model selection
  - Calls appropriate video service
  - Returns video file + metadata
  - **Features**: Easy model swapping, latency tracking, JSON metadata logging

### 4. **Utilities** (`src/utils/`)

- **`file_utils.py`**: Download files with timeout handling
- **`image_encoding.py`**: Convert local images to data URIs (base64)
- **`json_utils.py`**: JSON save/load helpers

### 5. **Clients** (`src/clients/`)

- **`fal_client.py`**: FAL API wrapper with retry logic and logging

---

## âš™ï¸ Configuration

### Environment Variables

```bash
export FAL_API_KEY="your_fal_api_key"
```

### Model Costs (`configs/model_costs.py`)

Track pricing per model (optional):

```python
MODEL_COSTS = {
    "fal-ai/nano-banana": 0.01,
    "fal-ai/veo3/image-to-video": 0.05,
    # ... add per-model costs
}
```

### Duration Validation (`src/schemas/generation.py`)

Edit `VideoGenerationRequest.validate_duration()` to support new durations:

```python
supported = [4, 5, 6, 8, 9]  # Adjust per model requirements
```

---

## ğŸ“Š Dashboard

**Script:** `scripts/dashboard.py`

**Features:**

1. **Metrics**
   - Total videos generated
   - Average latency per model
   - Number of models tested

2. **Charts**
   - Bar chart: Average latency by model
   - Box plot: Latency distribution
   - Scatter plot: Timeline (generation time vs latency)

3. **Tables**
   - Detailed statistics (min/avg/max latency, count)
   - All generated videos with timestamps

4. **Inspector**
   - View full raw metadata JSON for any video
   - Inspect API response details

5. **Export**
   - Download as CSV (latency comparison)
   - Download as JSON (full summary)

**Run:**
```bash
streamlit run scripts/dashboard.py
```

---

## ğŸ”Œ API Reference

### ImagePipeline

```python
from src.services.pipelines.image_pipeline import ImagePipeline
from src.schemas.person import PersonAttributes
from src.schemas.environment import EnvironmentAttributes

pipeline = ImagePipeline()

result = pipeline.run(
    person=PersonAttributes(height_cm=175, weight_kg=70, gender="male", age=28),
    env=EnvironmentAttributes(
        apparel_type="Street wear",
        inferred_setting="Urban cafe outdoor",
        visual_cues="Natural lighting"
    ),
    description="Full frontal view of athletic male",
    person_reference_image="assets/person.png",
    face_reference_image="assets/face_crop.png",  # Optional
    outfit_reference_images=["assets/top.png", "assets/bottom.png"]  # Optional
)

# result = {
#     "stage": "combined",
#     "raw": {...},  # Raw API response
#     "local_files": ["outputs/images/2026_02_04/nano_banana_edit/img_1770197158_0.png"],
#     "metadata_file": "outputs/images/2026_02_04/nano_banana_edit/meta_1770197158.json",
#     "latency_sec": 29.57
# }
```

### VideoPipeline

```python
from src.services.pipelines.video_pipeline import VideoPipeline

# Swap models: veo3, ltx, kling, grok, luma, pika, seedance, hunyuan
video_pipeline = VideoPipeline(video_model="veo3")

result = video_pipeline.run(
    reference_image="outputs/images/2026_02_04/nano_banana_edit/img_1770197158_0.png",
    apparel_description="blue t-shirt and khaki shorts",
    duration_sec=4
)

# result = {
#     "stage": "video",
#     "video_model": "veo3",
#     "raw": {...},
#     "local_files": ["outputs/videos/2026_02_04/veo3/video_1770197200.mp4"],
#     "metadata_file": "outputs/videos/2026_02_04/veo3/meta_1770197200.json",
#     "latency_sec": 28.5
# }
```

### FalClient

```python
from src.clients.fal_client import FalClient

client = FalClient()

result = client.subscribe(
    model="fal-ai/nano-banana/edit",
    arguments={
        "prompt": "A person wearing blue shirt",
        "image_urls": ["data:image/png;base64,...", "..."],
        "resolution": "1024x1024",
        "num_images": 1
    }
)
```

---

## ğŸ”§ Development

### Adding a New Video Model

1. **Create service:**
   ```python
   # filepath: src/services/video_generation/newmodel_service.py
   from src.clients.fal_client import FalClient
   from src.schemas.generation import VideoGenerationRequest
   
   class NewmodelVideoService:
       MODEL_NAME = "vendor/newmodel/image-to-video"
       
       def __init__(self):
           self.client = FalClient()
       
       def generate_video(self, req: VideoGenerationRequest) -> dict:
           # 1. Resolve reference image
           # 2. Build arguments dict
           # 3. Call FAL model
           # 4. Download video
           # 5. Save metadata
           # 6. Return dict with local_files, metadata_file, latency_sec
           pass
   ```

2. **Register in pipeline:**
   ```python
   # src/services/pipelines/video_pipeline.py
   def _init_video_service(self, model_name: str):
       if model_name == "newmodel":
           return NewmodelVideoService()
       # ...
   ```

3. **Use in script:**
   ```python
   video_pipeline = VideoPipeline(video_model="newmodel")
   ```

### File Structure

```
felix/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â””â”€â”€ fal_client.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ image_generation/
â”‚   â”‚   â”‚   â”œâ”€â”€ nano_banana_service.py
â”‚   â”‚   â”‚   â””â”€â”€ nano_banana_edit_service.py
â”‚   â”‚   â”œâ”€â”€ video_generation/
â”‚   â”‚   â”‚   â”œâ”€â”€ veo3_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ltx_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ kling_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ grok_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ luma_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ pika_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ seedance_service.py
â”‚   â”‚   â”‚   â””â”€â”€ hunyuan_service.py
â”‚   â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”‚   â”œâ”€â”€ image_pipeline.py
â”‚   â”‚   â”‚   â””â”€â”€ video_pipeline.py
â”‚   â”‚   â””â”€â”€ prompt_builder/
â”‚   â”‚       â””â”€â”€ image_prompt_service.py
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ person.py
â”‚   â”‚   â”œâ”€â”€ environment.py
â”‚   â”‚   â”œâ”€â”€ generation.py
â”‚   â”‚   â””â”€â”€ artifacts.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”‚   â”œâ”€â”€ image_encoding.py
â”‚   â”‚   â””â”€â”€ json_utils.py
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_full_pipeline.py
â”‚   â”œâ”€â”€ run_image_generation.py
â”‚   â”œâ”€â”€ test_nanobanana.py
â”‚   â”œâ”€â”€ test_fal_connection.py
â”‚   â””â”€â”€ dashboard.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ model_costs.py
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ pranay.png
â”‚   â”œâ”€â”€ top.png
â”‚   â”œâ”€â”€ bottom.png
â”‚   â””â”€â”€ shoes.png
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ YYYY_MM_DD/
â”‚   â”‚       â””â”€â”€ nano_banana_edit/
â”‚   â”‚           â”œâ”€â”€ img_<ts>.png
â”‚   â”‚           â””â”€â”€ meta_<ts>.json
â”‚   â””â”€â”€ videos/
â”‚       â””â”€â”€ YYYY_MM_DD/
â”‚           â”œâ”€â”€ veo3/
â”‚           â”œâ”€â”€ ltx/
â”‚           â”œâ”€â”€ kling/
â”‚           â”œâ”€â”€ grok/
â”‚           â”œâ”€â”€ luma/
â”‚           â”œâ”€â”€ pika/
â”‚           â”œâ”€â”€ seedance/
â”‚           â””â”€â”€ hunyuan/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸ“ Metadata JSON Structure

**Image metadata** (`outputs/images/2026_02_04/nano_banana_edit/meta_*.json`):

```json
{
  "prompt": "Full frontal view...",
  "model": "fal-ai/nano-banana/edit",
  "seed": null,
  "latency_sec": 29.57,
  "timestamp": "2026-02-04T10:38:11.831988",
  "reference_images": ["assets/pranay.png", "assets/top.png", "assets/bottom.png"],
  "raw_response": { "images": [...] }
}
```

**Video metadata** (`outputs/videos/2026_02_04/veo3/meta_*.json`):

```json
{
  "prompt": "A guy standing in a proper lighting...",
  "model": "fal-ai/veo3/image-to-video",
  "duration_sec": 4,
  "reference_image": "outputs/images/2026_02_04/nano_banana_edit/img_1770197158_0.png",
  "latency_sec": 28.5,
  "timestamp": "2026-02-04T10:39:00.000000",
  "raw_response": {
    "video": {
      "url": "https://...",
      "content_type": "video/mp4",
      "width": 720,
      "height": 720,
      "fps": 24.0,
      "duration": 4.5
    }
  }
}
```

---

## ğŸ¤ Contributing

### Bug Reports & Feature Requests

File issues with:
- Error logs
- Steps to reproduce
- Expected vs. actual behavior

### Extending the Pipeline

See [Development](#development) section for adding new models.

---

## ğŸ“„ License

Proprietary. All rights reserved.

---

## ğŸ†˜ Support

- **FAL.ai Docs**: https://docs.fal.ai
- **API Status**: https://status.fal.ai
- **Issues**: Check logs in `outputs/` metadata JSONs

---

## ğŸ¯ Roadmap

- [ ] Support for streaming/real-time video generation
- [ ] Batch processing (multiple people/outfits)
- [ ] Cost analysis dashboard
- [ ] Quality metrics (SSIM, LPIPS, etc.)
- [ ] Multi-camera angle generation
- [ ] Voice/audio sync support
- [ ] Web UI (FastAPI + React)

---

**Version:** 0.1.0  
**Last Updated:** 2026-02-04  
**Author:** Felix Team